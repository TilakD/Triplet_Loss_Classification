{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check t-sne on Person 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import model.create_dataset as create_dataset\n",
    "from model.utils import Params\n",
    "from model.model_fn import model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"experiments/p1_training\"\n",
    "landmark_dir = \"data_for_model_person_2/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset(landmark_dir, params, class_dict_dir):\n",
    "\n",
    "    dataset = create_dataset.dataset(landmark_dir, params, class_dict_dir)\n",
    "    dataset = dataset.batch(params.batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _get_dataset_size(landmark_dir, image_type):\n",
    "\n",
    "\tsize = 0\n",
    "\tfor root, dirs, files in os.walk(landmark_dir):\n",
    "\t    files = [f for f in files if \".\"+image_type in f ]\n",
    "\t    size += len(files)\n",
    "\n",
    "\ttf.logging.info(\"Found {} {} landmarks in {}\".format(size, image_type, landmark_dir))\n",
    "\n",
    "\treturn size\n",
    "\n",
    "\n",
    "def _get_embeddings(landmark_dir, estimator, params, class_dict_dir, landmark_size):\n",
    "        \n",
    "    # Compute embeddings\n",
    "    tf.logging.info(\"Predicting on \"+landmark_dir)    \n",
    "\n",
    "    predictions = estimator.predict(lambda: _get_dataset(landmark_dir, params, class_dict_dir))\n",
    "\n",
    "    embeddings = np.zeros((landmark_size, params.embedding_size))\n",
    "    for i, p in enumerate(predictions):\n",
    "        embeddings[i] = p['embeddings']\n",
    "\n",
    "    tf.logging.info(\"Embeddings shape in \"+os.path.basename(landmark_dir)+\": {}\".format(embeddings.shape))\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load the parameters from json file\n",
    "    json_path = os.path.join(model_dir, 'params.json')\n",
    "    assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "    params = Params(json_path)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # Define the model\n",
    "    tf.logging.info(\"Creating the model...\")\n",
    "    config = tf.estimator.RunConfig(tf_random_seed=230,\n",
    "                                    model_dir=model_dir,\n",
    "                                    save_summary_steps=params.save_summary_steps)\n",
    "    estimator = tf.estimator.Estimator(model_fn, params=params, config=config)\n",
    "\n",
    "    # Create a new folder to save embeddings\n",
    "    embeddings_dir = os.path.join(model_dir, \"landmarks\")\n",
    "    if not os.path.exists(embeddings_dir):\n",
    "        os.makedirs(embeddings_dir)\n",
    "\n",
    "    # Get the number of landmarks\n",
    "    landmark_size = _get_dataset_size(os.path.normpath(landmark_dir), params.image_type)\n",
    "\n",
    "    # Get embeddings and define tensorflow variables\n",
    "    embeddings = _get_embeddings(landmark_dir, estimator, params, embeddings_dir, landmark_size)\n",
    "    embeddings = np.round(embeddings, 6)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.032304, -0.013719, -0.002834, ..., -0.046816, -0.026536,\n",
       "        -0.033028],\n",
       "       [ 0.032123, -0.01372 , -0.003483, ..., -0.0451  , -0.027991,\n",
       "        -0.03247 ],\n",
       "       [ 0.032567, -0.013745, -0.003429, ..., -0.045663, -0.027912,\n",
       "        -0.032821],\n",
       "       ...,\n",
       "       [ 0.032138, -0.014209, -0.003598, ..., -0.045272, -0.027055,\n",
       "        -0.033075],\n",
       "       [ 0.033194, -0.015614, -0.003484, ..., -0.045428, -0.027075,\n",
       "        -0.032875],\n",
       "       [ 0.033047, -0.014464, -0.0044  , ..., -0.0447  , -0.026596,\n",
       "        -0.033183]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "s='jpg'\n",
    "a=\"jpeg\"    \n",
    "b=\"JPG\" \n",
    "c=\"png\" \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in os.listdir(path):\n",
    "        #print(i)\n",
    "        for f in os.listdir(os.path.join(path, i)):\n",
    "             if f.endswith(s) or f.endswith(a)  or f.endswith(b) or f.endswith(c):\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "    return np.array(metadata)\n",
    "\n",
    "metadata = load_metadata(landmark_dir)\n",
    "#print(metadata)\n",
    "print(metadata.shape[0])\n",
    "\n",
    "targets = np.array([m.name for m in metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "X_embedded = TSNE(n_components=2).fit_transform(embeddings)\n",
    "#print(X_embedded)\n",
    "fig=plt.figure(figsize=(15,8))\n",
    "for i, t in enumerate(set(targets)):\n",
    "    idx = targets == t\n",
    "    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1));\n",
    "fig.savefig(\"cluster_500_epoch.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check t-sne on Person 1, 2, 3 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"experiments/p1_training\"\n",
    "landmark_dir = \"Images/p1_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load the parameters from json file\n",
    "    json_path = os.path.join(model_dir, 'params.json')\n",
    "    assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "    params = Params(json_path)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # Define the model\n",
    "    tf.logging.info(\"Creating the model...\")\n",
    "    config = tf.estimator.RunConfig(tf_random_seed=230,\n",
    "                                    model_dir=model_dir,\n",
    "                                    save_summary_steps=params.save_summary_steps)\n",
    "    estimator = tf.estimator.Estimator(model_fn, params=params, config=config)\n",
    "\n",
    "    # Create a new folder to save embeddings\n",
    "    embeddings_dir = os.path.join(model_dir, \"landmarks\")\n",
    "    if not os.path.exists(embeddings_dir):\n",
    "        os.makedirs(embeddings_dir)\n",
    "\n",
    "    # Get the number of landmarks\n",
    "    landmark_size = _get_dataset_size(os.path.normpath(landmark_dir), params.image_type)\n",
    "\n",
    "    # Get embeddings and define tensorflow variables\n",
    "    embeddings = _get_embeddings(landmark_dir, estimator, params, embeddings_dir, landmark_size)\n",
    "    embeddings = np.round(embeddings, 6)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "s='jpg'\n",
    "a=\"jpeg\"    \n",
    "b=\"JPG\" \n",
    "c=\"png\" \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in os.listdir(path):\n",
    "        #print(i)\n",
    "        for f in os.listdir(os.path.join(path, i)):\n",
    "             if f.endswith(s) or f.endswith(a)  or f.endswith(b) or f.endswith(c):\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "    return np.array(metadata)\n",
    "\n",
    "metadata = load_metadata(landmark_dir)\n",
    "#print(metadata)\n",
    "print(metadata.shape[0])\n",
    "\n",
    "targets = np.array([m.name for m in metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "X_embedded = TSNE(n_components=2).fit_transform(embeddings)\n",
    "#print(X_embedded)\n",
    "fig=plt.figure(figsize=(15,8))\n",
    "for i, t in enumerate(set(targets)):\n",
    "    idx = targets == t\n",
    "    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1));\n",
    "fig.savefig(\"cluster_p1_2_3_5.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
